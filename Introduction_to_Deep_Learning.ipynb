{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Introduction to Deep Learning Assignment questions."
      ],
      "metadata": {
        "id": "sAQdFGuV-LLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Explain what deep learning is and discuss its significance in the broader field of artificial intelligence.\n",
        "# Ans: Deep learning is a subset of machine learning, which itself is a subset of artificial intelligence (AI). Deep learning models are designed to mimic the way the human brain processes information using structures called artificial neural networks. These networks consist of layers of interconnected nodes (neurons) that process and transform input data to extract meaningful patterns and representations.\n",
        "\n",
        "# Key features of deep learning include:\n",
        "\n",
        "# Hierarchical Representation: It automatically extracts features from raw data, building higher-level abstractions through multiple processing layers.\n",
        "# End-to-End Learning: Deep learning models can process raw data directly and learn complex mappings from input to output.\n",
        "# Scalability: They work effectively with large datasets and can leverage significant computational resources, such as GPUs.\n",
        "\n",
        "# Core Components of Deep Learning:\n",
        "# Neural Networks: The foundation of deep learning, structured in layers (input, hidden, and output).\n",
        "# Activation Functions: Non-linear transformations applied to the data as it passes through a network.\n",
        "# Backpropagation: A method used to optimize the network by minimizing error through gradient descent.\n",
        "# Loss Functions: Quantify how well the model's predictions match the actual values.\n",
        "\n",
        "# Significance of Deep Learning in AI\n",
        "# Enhanced Performance:\n",
        "\n",
        "# Deep learning algorithms consistently outperform traditional machine learning approaches in tasks involving unstructured data (e.g., images, audio, text).\n",
        "# Examples include computer vision, speech recognition, and natural language processing (NLP).\n",
        "# Automation of Feature Engineering:\n",
        "\n",
        "# Unlike traditional machine learning methods that require manual feature extraction, deep learning models automatically discover relevant features during training.\n",
        "# Applications Across Domains:\n",
        "\n",
        "# Healthcare: Medical image analysis (e.g., detecting lung cancer, diabetic retinopathy).\n",
        "# Finance: Fraud detection, algorithmic trading.\n",
        "# Entertainment: Recommendation systems (e.g., Netflix, Spotify).\n",
        "# Autonomous Systems: Self-driving cars, robotics.\n",
        "# Scalability:\n",
        "\n",
        "# Deep learning models thrive with large-scale data, leveraging massive datasets to uncover intricate patterns that simpler models cannot.\n",
        "# Advancing AI Research:\n",
        "\n",
        "# Technologies like GPT (Generative Pre-trained Transformer) and DALL¬∑E are powered by deep learning, revolutionizing areas like creative AI and language understanding."
      ],
      "metadata": {
        "id": "jseY-M4Q-SHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. List and explain the fundamental components of artificial neural networks.\n",
        "# Ans: Artificial Neural Networks (ANNs) are computational models inspired by the structure and function of biological neural networks. They are made up of several interconnected components that work together to process information and learn patterns from data. Below are the fundamental components of ANNs:\n",
        "\n",
        "# 1. Input Layer:\n",
        "# Purpose: The input layer receives raw data or features from the external environment.\n",
        "# Structure: Each neuron in this layer corresponds to one feature of the input data.\n",
        "# Example: In an image recognition task, pixel values might serve as inputs.\n",
        "\n",
        "# 2. Hidden Layers:\n",
        "# Purpose: Hidden layers perform intermediate computations to learn complex patterns and relationships in the data.\n",
        "# Structure: Consist of neurons that receive weighted inputs from the previous layer, apply a transformation (via an activation function), and pass the result to the next layer.\n",
        "# Importance: The \"depth\" of an ANN (i.e., the number of hidden layers) determines its ability to model complex patterns.\n",
        "\n",
        "# 3. Output Layer:\n",
        "# Purpose: Produces the final prediction or output of the network.\n",
        "# Structure: The number of neurons in the output layer depends on the type of task:\n",
        "# Regression: One neuron for a continuous value.\n",
        "# Binary Classification: One neuron with a sigmoid activation function.\n",
        "# Multiclass Classification: One neuron per class with a softmax activation function.\n",
        "\n",
        "# 4. Neurons:\n",
        "# Purpose: The basic processing units of an ANN.\n",
        "# Function: Each neuron computes the weighted sum of its inputs, adds a bias term, applies an activation function, and sends the result to the next layer.\n",
        "\n",
        "# 5. Weights:\n",
        "# Purpose: Weights determine the strength and direction of the connection between two neurons.\n",
        "# Role: During training, the network learns optimal weights to minimize prediction errors.\n",
        "\n",
        "# 6. Bias:\n",
        "# Purpose: A bias term is added to the weighted sum to shift the activation function and improve flexibility.\n",
        "# Role: It helps the model fit the data more accurately by adjusting the output independently of the input.\n",
        "\n",
        "# 7. Activation Functions:\n",
        "# Purpose: Introduce non-linearity into the network to allow it to learn complex patterns.\n",
        "# Common Types:\n",
        "# Sigmoid: Outputs values between 0 and 1, often used in binary classification.\n",
        "# ReLU (Rectified Linear Unit): Replaces negative values with zero, commonly used in hidden layers.\n",
        "# Tanh: Outputs values between -1 and 1, useful for centered data.\n",
        "# Softmax: Converts outputs into probabilities for multiclass classification.\n",
        "\n",
        "# 8. Forward Propagation:\n",
        "# Purpose: Passes input data through the network to compute the output.\n",
        "# Process: Involves calculating weighted sums, applying activation functions, and propagating the results layer by layer.\n",
        "\n",
        "# 9. Loss Function:\n",
        "# Purpose: Measures how well the network's predictions match the actual targets.\n",
        "# Common Types:\n",
        "# Mean Squared Error (MSE): Used for regression tasks.\n",
        "# Binary Cross-Entropy: Used for binary classification tasks.\n",
        "# Categorical Cross-Entropy: Used for multiclass classification.\n",
        "\n",
        "# 10. Backpropagation:\n",
        "# Purpose: Adjusts weights and biases based on the gradient of the loss function to reduce errors.\n",
        "# Process: Involves calculating gradients using the chain rule and updating parameters through optimization algorithms.\n",
        "\n",
        "# 11. Optimization Algorithms:\n",
        "# Purpose: Update weights and biases to minimize the loss function during training.\n",
        "# Common Types:\n",
        "# Gradient Descent: Iteratively reduces the loss by adjusting weights in the direction of the negative gradient.\n",
        "# Variants: Stochastic Gradient Descent (SGD), Adam, RMSProp.\n",
        "\n",
        "# 12. Learning Rate:\n",
        "# Purpose: Controls the size of the steps taken during weight updates.\n",
        "# Significance: A properly chosen learning rate ensures faster convergence without overshooting.\n",
        "\n",
        "# 13. Epochs and Batch Size:\n",
        "# Epoch: A single pass through the entire training dataset.\n",
        "# Batch Size: The number of samples processed before updating the model's weights."
      ],
      "metadata": {
        "id": "PNxFwelL-8io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Discuss the roles of neurons, connections, weights, and biases.\n",
        "# Ans: In artificial neural networks (ANNs), neurons, connections, weights, and biases form the core elements of the model's architecture, each playing a critical role in how the network processes and learns from data. Here's a breakdown of their roles:\n",
        "\n",
        "# 1. Neurons\n",
        "# Role: Neurons are the fundamental processing units of an ANN, analogous to biological neurons in the human brain.\n",
        "# Functionality:\n",
        "# Each neuron receives inputs, processes them, and produces an output that is passed to the neurons in the next layer.\n",
        "# The processing involves a weighted sum of the inputs, adding a bias, and applying an activation function.\n",
        "# Importance:\n",
        "# Neurons allow the network to compute transformations and model relationships in the data.\n",
        "# They work collectively to extract features, detect patterns, and make predictions.\n",
        "# 2. Connections\n",
        "# Role: Connections link neurons across layers, enabling the flow of information through the network.\n",
        "# Functionality:\n",
        "# Each connection has an associated weight that determines the influence of the source neuron's output on the target neuron.\n",
        "# Connections represent the pathways along which information propagates during forward and backward passes.\n",
        "# Importance:\n",
        "# The network's complexity and capacity to model relationships depend on the connectivity between neurons.\n",
        "# Dense (fully connected) layers ensure that every neuron in one layer is linked to every neuron in the next layer.\n",
        "# 3. Weights\n",
        "# Role: Weights are the adjustable parameters that control the strength and direction of the influence between connected neurons.\n",
        "# Functionality:\n",
        "# A weight is multiplied by the output of a neuron before it is passed to the next neuron.\n",
        "# During training, weights are updated using optimization algorithms (e.g., gradient descent) to minimize the error of predictions.\n",
        "# Significance:\n",
        "# Weights capture the learned patterns and relationships in the data.\n",
        "# Their optimization is central to the network's ability to generalize and make accurate predictions.\n",
        "# 4. Biases\n",
        "# Role: Biases allow the network to shift the output of a neuron independently of its inputs, providing additional flexibility in learning.\n",
        "# Functionality:\n",
        "# A bias is a constant term added to the weighted sum of inputs before applying the activation function.\n",
        "# It enables the network to better fit the data, especially when the inputs alone cannot explain the target output.\n",
        "# Significance:\n",
        "# Without biases, the network's output would be constrained, and it might struggle to model data where relationships aren't centered around the origin (0, 0).\n",
        "# Biases improve the network's ability to capture patterns that require an offset."
      ],
      "metadata": {
        "id": "OMWXmDc9_vZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Illustrate the architecture of an artificial neural network. Provide an example to explain the flow of information through the network.\n",
        "# Ans: The architecture of an ANN typically consists of three main layers:\n",
        "\n",
        "# Input Layer: Receives the raw data (features) from the environment.\n",
        "# Hidden Layers: Perform intermediate computations to extract patterns and transform data.\n",
        "# Output Layer: Produces the final output of the network (e.g., predictions or classifications).\n",
        "# Each layer contains neurons connected by weighted connections, and each connection may have a bias added. Non-linear activation functions are applied to introduce flexibility in modeling complex relationships.\n",
        "\n",
        "# Example ANN Architecture\n",
        "# Below is a simplified illustration of a fully connected ANN with:\n",
        "\n",
        "# 3 input features\n",
        "# 1 hidden layer with 4 neurons\n",
        "# 1 output layer with 2 neurons (e.g., for binary classification)\n",
        "\n",
        "# Input Layer       Hidden Layer (4 Neurons)     Output Layer (2 Neurons)\n",
        "#  [x‚ÇÅ] --------> [h‚ÇÅ] ----> [y‚ÇÅ]\n",
        "#  [x‚ÇÇ] ----|     [h‚ÇÇ] ----> [y‚ÇÇ]\n",
        "#  [x‚ÇÉ] ----|     [h‚ÇÉ]\n",
        "#             |    [h‚ÇÑ]\n"
      ],
      "metadata": {
        "id": "leitG5j4AHeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Outline the perceptron learning algorithm. Describe how weights are adjusted during the learning process.\n",
        "# Ans: The Perceptron Learning Algorithm is a fundamental supervised learning algorithm for binary classification. It adjusts weights iteratively to minimize classification errors, ensuring that the perceptron correctly separates the two classes in a linearly separable dataset.\n",
        "\n",
        "# Weight Adjustment During Learning\n",
        "# The weights are adjusted based on the following logic:\n",
        "\n",
        "# Misclassified Example: if y = +1 and y^ = -1 (underestimation):\n",
        "# The weights are increased, shifting the decision boundary closer to the example.\n",
        "# If y=‚àí1 and y^ = +1 (overestimation):\n",
        "# The weights are decreased, shifting the decision boundary away from the example.\n",
        "# Correctly Classified Example:\n",
        "# No weight updates are performed.\n",
        "# The adjustment ensures that the perceptron learns to \"move\" the decision boundary until it separates the two classes."
      ],
      "metadata": {
        "id": "RhkiLrCpByDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Discuss the importance of activation functions in the hidden layers of a multi-layer perceptron. Provide examples of commonly used activation functions.\n",
        "# Ans: In a multi-layer perceptron (MLP), activation functions play a critical role by introducing non-linearity into the network. This enables the MLP to model complex, non-linear relationships between inputs and outputs, which is essential for solving real-world problems.\n",
        "\n",
        "# Why Activation Functions Are Important\n",
        "# Non-Linearity:\n",
        "\n",
        "# Without activation functions, the network behaves like a linear transformation, regardless of the number of layers.\n",
        "# Non-linearity allows the network to learn and approximate non-linear mappings, which are crucial for tasks like image recognition, language translation, and more.\n",
        "# Feature Transformation:\n",
        "\n",
        "# Activation functions transform raw outputs of neurons into a format suitable for the next layer.\n",
        "# This helps in capturing intricate patterns in data.\n",
        "# Gradient-Based Optimization:\n",
        "\n",
        "# Activation functions determine how errors are propagated back during backpropagation.\n",
        "# Smooth, differentiable activation functions ensure that gradients can be computed and used to update weights.\n",
        "# Output Scaling:\n",
        "\n",
        "# Activation functions can squash values into a specific range (e.g., 0 to 1, -1 to 1), making them interpretable or suitable for downstream processing.\n",
        "\n",
        "# Commonly Used Activation Functions:\n",
        "\n",
        "# Sigmoid Function:\n",
        "# f (x) = 1/1+e^-x\n",
        "# Range: 0 to 1\n",
        "# Characteristics:\n",
        "# S-shaped curve.\n",
        "# Used for binary classification problems.\n",
        "# Challenges: Saturates at extremes (vanishing gradients) and computationally expensive.\n",
        "# Applications: Output layers in binary classification.\n",
        "\n",
        "# Hyperbolic Tangent (Tanh):\n",
        "# f (x) = e^x - e^-x / e^x + e^-x\n",
        "\n",
        "# Range: -1 to 1\n",
        "# Characteristics:\n",
        "# Centered at zero, which helps optimization converge faster.\n",
        "# Suffers from vanishing gradient for large values of ùë•\n",
        "# Applications: Hidden layers in some cases, especially in sequential data.\n",
        "\n",
        "# Rectified Linear Unit (ReLU):\n",
        "# f (x) = max(0, x)\n",
        "\n",
        "# Range: [0,‚àû)\n",
        "# Characteristics:\n",
        "# Introduces sparsity by outputting\n",
        "# 0 for negative inputs.\n",
        "# Efficient to compute and does not saturate for positive inputs.\n",
        "# Challenges: Can cause \"dead neurons\" (outputs stuck at zero).\n",
        "# Applications: Hidden layers in most modern deep learning architectures."
      ],
      "metadata": {
        "id": "pYh6w9KUCtG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Various Neural Network Architect Overview Assignments:"
      ],
      "metadata": {
        "id": "uZmla2E4EO6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Describe the basic structure of a Feedforward Neural Network (FNN). What is the purpose of the activation function?\n",
        "# Ans: A Feedforward Neural Network (FNN) is the simplest type of artificial neural network. It consists of layers of neurons where the information moves in a single direction: from the input layer, through hidden layers, to the output layer. There are no loops or cycles in the network.\n",
        "\n",
        "# Components of a Feedforward Neural Network\n",
        "# Input Layer:\n",
        "\n",
        "# Receives the input data as features (x1, x2, ..... xn)\n",
        "# Each neuron represents one input feature.\n",
        "# This layer does not perform computations, it just passes the data to the next layer.\n",
        "# Hidden Layers:\n",
        "\n",
        "# These layers transform the input data by applying weights, biases, and activation functions.\n",
        "# There can be one or more hidden layers.\n",
        "# Hidden layers extract intermediate features and help the network learn complex patterns.\n",
        "# Output Layer:\n",
        "\n",
        "# Produces the final result of the network (e.g., classification label or regression output).\n",
        "# The number of neurons depends on the specific task:\n",
        "# Binary classification: One neuron (with sigmoid activation).\n",
        "# Multi-class classification: One neuron per class (with softmax activation).\n",
        "# Regression: One neuron (with linear activation).\n",
        "\n",
        "# Purpose of the Activation Function\n",
        "# The activation function is applied to the output of each neuron in the hidden and output layers to introduce non-linearity and transform the data. It is a crucial component that enables the network to solve complex, non-linear problems.\n",
        "\n",
        "# Key Roles of the Activation Function:\n",
        "# Non-Linearity:\n",
        "\n",
        "# Real-world problems are often non-linear. Activation functions allow the network to approximate these relationships.\n",
        "# Feature Transformation:\n",
        "\n",
        "# They transform the weighted sum of inputs into a meaningful output that can be processed by subsequent layers.\n",
        "# Thresholding:\n",
        "\n",
        "# Functions like sigmoid or ReLU can \"activate\" or \"suppress\" neurons, deciding which features are passed forward.\n",
        "# Gradient-Based Optimization:\n",
        "\n",
        "# Differentiable activation functions enable the use of backpropagation to update weights during training."
      ],
      "metadata": {
        "id": "BCrlAq82ER96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Explain the role of convolutional layers in CNN. Why are pooling layers commonly used, and what do they achieve?\n",
        "# Ans: The convolutional layer is the core building block of a Convolutional Neural Network (CNN). Its primary purpose is to extract features from the input data (e.g., images, videos) using a mathematical operation called convolution.\n",
        "\n",
        "# Why Pooling Layers Are Used:\n",
        "# Dimensionality Reduction:\n",
        "\n",
        "# Reduces the size of feature maps, lowering computational cost and memory usage.\n",
        "# Focus on Dominant Features:\n",
        "\n",
        "# Highlights the most relevant features by down-sampling.\n",
        "# Discards noise and less significant information.\n",
        "# Translation Invariance:\n",
        "\n",
        "# Ensures that the network focuses on high-level features rather than precise locations of patterns.\n",
        "# Prevent Overfitting:\n",
        "\n",
        "# Reducing the complexity of the model helps prevent overfitting to the training data.\n",
        "\n",
        "# Pooling Layers: Down-sample feature maps to reduce computational cost, enhance robustness, and focus on critical features. Together, these layers form the backbone of CNNs, enabling them to excel in image processing, computer vision, and related tasks."
      ],
      "metadata": {
        "id": "9S535Qb6FHR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. What is the key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural networks? How does an RNN handle sequential data?\n",
        "# Ans: The key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural networks is their ability to handle sequential data by incorporating a memory mechanism. Unlike feedforward neural networks, which assume independence between inputs, RNNs retain information from previous inputs and use it to influence the current output.\n",
        "# This is achieved through recurrent connections that allow information to persist across time steps. These connections enable RNNs to model temporal dependencies, making them suitable for tasks where the order of data matters, such as time series, natural language, or audio.\n",
        "\n",
        "# How an RNN Handles Sequential Data\n",
        "# Recurrent Structure:\n",
        "\n",
        "# RNNs process data sequentially, one time step at a time. At each time step ùë°, the network takes two inputs:\n",
        "# The current input (ùë•ùë°)\n",
        "# The hidden state (‚Ñéùë°‚àí1) from the previous time step, which acts as memory.\n",
        "\n",
        "# Advantages of RNNs for Sequential Data\n",
        "# Temporal Dependency:\n",
        "# RNNs capture dependencies between elements in a sequence, making them ideal for tasks like language modeling, speech recognition, and stock price prediction.\n",
        "# Variable Input Length:\n",
        "# Unlike fixed-size inputs required in other architectures, RNNs can handle variable-length sequences."
      ],
      "metadata": {
        "id": "xDscwpGbFl6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Discuss the components of a Long Short-Term Memory (LSTM) network. How does it address the vanishing gradient problem?\n",
        "# Ans: Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) designed to overcome some of the challenges faced by standard RNNs, particularly the vanishing gradient problem. LSTMs are equipped with special mechanisms that allow them to maintain and update a memory cell over long sequences of data.\n",
        "# An LSTM consists of several key components that work together to control the flow of information through the network:\n",
        "\n",
        "# Key Components of an LSTM\n",
        "# Cell State:\n",
        "\n",
        "# The cell state (ùê∂ùë°) is the central component of the LSTM that carries long-term information across time steps. It acts as the \"memory\" of the network and is modified by the gates at each time step.\n",
        "# The cell state is updated using the outputs of the gates and remains relatively unchanged unless explicitly modified by these gates.\n",
        "# Gates: LSTM networks use three gates that control the flow of information into and out of the cell state. These gates are essentially neural networks that decide how much information should be allowed to pass through the network at each time step.\n",
        "\n",
        "# Addressing the Vanishing Gradient Problem\n",
        "# The vanishing gradient problem occurs in traditional RNNs when gradients become very small as they are propagated backward through time. This makes it difficult for the network to learn long-term dependencies, as the weights associated with earlier time steps are updated very little (or not at all).\n",
        "\n",
        "# LSTMs address the vanishing gradient problem in several ways:\n",
        "\n",
        "# Cell State:\n",
        "\n",
        "# The cell state\n",
        "# acts as a constant error carousel, which allows gradients to flow unchanged across time steps. This makes it easier to preserve long-term dependencies because the cell state is updated with minimal modification by the forget and input gates, allowing it to maintain important information across many time steps.\n",
        "# Unlike traditional RNNs, where information decays rapidly, LSTMs can carry gradients across longer sequences without them diminishing.\n",
        "# Gated Mechanisms:\n",
        "\n",
        "# The three gates (forget, input, and output) provide precise control over how information flows through the network. This enables the LSTM to decide what information to remember and what to forget at each time step, reducing the risk of unnecessary information decaying.\n",
        "# The forget gate, in particular, allows the LSTM to retain important long-term information and discard irrelevant data.\n",
        "# Gradients Flow More Easily:\n",
        "\n",
        "# The cell state passes through each time step with little modification, meaning that the gradient of the cell state is often close to 1, allowing for more stable learning over long sequences.\n",
        "# When backpropagating through time, the network is less likely to suffer from vanishing gradients because the information in the cell state is more likely to remain intact, making it easier to adjust weights over long periods."
      ],
      "metadata": {
        "id": "7oDcL9JVGzBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Describe the roles of the generator and discriminator in a Generative Adversarial Network (GAN). What is the training objective for each?\n",
        "# Ans: In a Generative Adversarial Network (GAN), two neural networks‚Äîthe generator and the discriminator‚Äîwork in tandem in a game-theoretic framework. The generator aims to create realistic data samples, while the discriminator attempts to distinguish between real and generated samples. They are trained simultaneously in a process that improves the performance of both.\n",
        "\n",
        "# Roles of the Generator and Discriminator\n",
        "# 1. Generator (G)\n",
        "# Role: The generator's primary job is to produce synthetic data (e.g., images, audio, text) that closely resembles real data.\n",
        "# Input: A random noise vector (ùëß) sampled from a known distribution (e.g., Gaussian or uniform distribution).\n",
        "# Output: A synthetic data sample (ùëß).\n",
        "# Training Objective: To generate samples that are so realistic that the discriminator cannot distinguish them from real data.\n",
        "# 2. Discriminator (D)\n",
        "# Role: The discriminator's job is to act as a binary classifier, distinguishing between real data (from the dataset) and fake data (produced by the generator).\n",
        "# Input: Data samples, either real or generated.\n",
        "# Output: A probability score indicating whether the input is real (1) or fake (0).\n",
        "# Training Objective: To correctly classify real and fake data by minimizing classification errors.\n",
        "# Training Objectives\n",
        "# GANs are trained using a minimax game where the generator and discriminator have opposing objectives:"
      ],
      "metadata": {
        "id": "0G6hECloJnUf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}